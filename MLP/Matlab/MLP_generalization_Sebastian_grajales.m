%Multilayer perceptron generalized code by: Sebastian Grajales Pulgarin (the apprentice)
clc; close all; clear all;
%Part of the data set generation: 
points= 2000; %Number of points of each class
class = input("Number of classes in the data set(<6): "); %Number of classes
X=[]; %Initialization of the vector with coordinates
y = []; %Initialization of the vector with class number
%Generation of data set points:
for class_number=1:class
    r = linspace(0.0, 3, points);  %Radius
    t = linspace(class_number*4, (class_number+1)*4, points) + randn(1,points)*0.08*0.18*class;
    A= ([r.*sin(t*2.5); r.*cos(t*2.5)]');
    X = vertcat(X,A);
    B = class_number*ones(points,1);
    y = vertcat(y,B);
end

numGraphs = class+2; % Variable that determines the total number of graphs

numrows = floor(sqrt(numGraphs )); % Number of rows
numcolumns = ceil(numGraphs / numrows); % Number of columns
subplot(numrows, numcolumns, 1); %Subplot to show all the graphs
scatter(X(:,1), X(:,2), [], y(:), 'filled'); %Graph of the training set
title("Training set");

Y = zeros(length(y), 1); %Initialize output value matrix
prev = y(1); %Prev value
col = 1; % Column counter
%Cycle to check the vector of class values and generate a new column if a different value is found.
for i = 1:length(y)
   % If the current value is different from the previous one, create a new column
    if y(i) ~= prev
        col = col + 1;
        prev = y(i);
    end
    % Add current value to the output matrix
    Y(i, col) = y(i);
end

b=-1; %Bias value
x = [b*ones(size(X,1),1),X]; %training set with bias
y =Y; %Expected values
idx = find(y>1);
y(idx) = 1; %Change class values to 1
data_set = [x,y];

%Step 1: Set the Network Parameters
f_a = @(x)tanh(x);%Activation function 
dev_a = @(x)1-tanh(x).^2;%Derived from the activation function 

hidden_layers = input("Number of hidden layers: "); 
W ={}; %Cell of the synaptic weights 
%Initialization of the network weights::
neurons = zeros(1,hidden_layers);
neurons(1) = input("Neurons# of the first hidden layer: "); %Neurons of the first layer
W{1} = rand(neurons(1),size(x,2))-0.5;
for i=2:hidden_layers
    neurons(i) = input("Neurons# hidden layer "+num2str(i)+": ");
    W{i}= rand(neurons(i),neurons(i-1)+1)-0.5;
end
W{hidden_layers+1} = rand(size(y,2),neurons(hidden_layers)+1)-0.5; %Last layer weights

nEpochs = input("Number of epochs: ");
eta_i = 0.001; %Learning parameter initial value 
eta_f = 0.0001; %Learning parameter final value 
emedio = {}; %Arrangement for storing the mean squared error
cte = 1; %value to display the current number of epoch
%Step 2: The algorithm
for epoch=1:nEpochs
   sumt=0; %Variable to store the sum of the mean squared errors generated by each sample 
   eta = eta_i +(eta_f-eta_i)*(epoch/nEpochs); %Linear descent rule for the learning parameter
   %Mixing training data
   data_set = data_set(randperm(size(data_set,1)), :);
   x = data_set(:, 1:3); % Training data
   y = data_set(:, 4:end); % Expected values
   %Cycle to present all data
   for j=1:(size(x,1))
   I = {}; %Initialization of the activation potentials cell
   Y={};%Initialization of the layers output cell
   delta = {}; %Initialization of the local error cell of each neuron 
   error = {}; %Error cell initialization 
   %Feedforward 
   Y{1} = x(j,:)'; %Output of the input layer
   I{1}=W{1}*Y{1}; %Activation potential of the first layer
   Y{2}=[b;f_a(I{1})]; %Output of the firts hidden layer
   for i=2:hidden_layers
      I{i}=W{i}*Y{i}; %Activation potential of layer(i)
      Y{i+1}=[b;f_a(I{i})]; %Hidden layer output(i)
   end 
   I{hidden_layers+1}=W{size(W,2)}*Y{size(Y,2)}; %Activation potential of the otput layer
   Y{hidden_layers+2}=[f_a(I{size(I,2)})];%Neural network output
   
   %back propagation:
   error{1}= y(j,:)'- Y{size(Y,2)};%Output layer error calculation
   delta{1}=(error{1}.*dev_a(I{size(I,2)}))'; %Responsibility for the error of each neuron in the output layer
   W_new = cell(1,size(W,2)); %Initialization cell with new weights
   W_new{size(W,2)}= W{size(W,2)}+(eta*delta{1}'*Y{hidden_layers+1}'); %Update weights between the last hidden layer and the output layer
   aux=0; %Auxiliary variable for traversing indexes in back propagation
   for k=2:hidden_layers+1%
       delta{k}=delta{size(delta,2)}*W{hidden_layers+1-aux}(:,2:end).*dev_a(I{hidden_layers-aux})'; %Responsibility for the error of each neuron in the layer (hidden_layers-aux)
       W_new{hidden_layers-aux}= W{hidden_layers-aux}+(eta*delta{size(delta,2)}'*Y{hidden_layers-aux}'); %Update weights between the layer (hidden_layers-aux) and the layer (hidden_layers-aux-1)   
       aux = aux+1;
   end 
    W = W_new; %Once the error is propagated, the weights to be used are updated with the following data
    sumt = sumt+0.5*sum(error{1}.^2); %Sum of specific errors
   end
   emedio{epoch}= sumt/size(x,1); %Root mean square error to measure network performance 
   if cte == 25 %conditional to display the current epoch
     disp(epoch)
     cte=0;
    end
    cte = cte+1;
end
%Network performance graph:
emedio = cell2mat(emedio);
subplot(numrows, numcolumns, 2);
plot(emedio)
title("Performance");
xlabel('epochs')
ylabel('MSE')
%Graphing zone of the network output:
n=200; %Number of points in our test network 
u = linspace(-4, 4, n);
v = linspace(-4, 4, n);
Z = {};
%Obtaining the output of our network before the mesh of test points 
for i=1:size(y,2)
    Z{i}=zeros([length(u),length(v)]);
end
for m=1:length(u)
    for j=1:length(v)
   I = {}; %Initialization of the activation potentials cell
   Y={};%Initialization of the layers output cell 
   %Feedforward 
   y_enter = [b,u(m),v(j)];
   I{1}=W{1}*y_enter'; %Activation potential of the first layer
   Y{1}=[b;f_a(I{1})]; %Output of the first hidden layer
   for i=2:hidden_layers
      I{i}=W{i}*Y{i-1}; %Activation potential of layer(i)
      Y{i}=[b;f_a(I{i})]; %Hidden layer output(i)
   end 
   I{hidden_layers+1}=W{size(W,2)}*Y{size(Y,2)};%Activation potential of the last layer
   Y{hidden_layers+1}=[f_a(I{size(I,2)})];%Neural network output
   for i=1:size(y,2)
   Z{i}(m,j)=Y{hidden_layers+1}(i);
    end
    end
end
%Cycle to plot the responses of the output neurons.
for i=1:size(y,2)
 subplot(numrows, numcolumns, 2+i);
 surf(u, v, cell2mat(Z(i))');
 title("Output Neuron "+num2str(i));
 shading interp;
end
shg

